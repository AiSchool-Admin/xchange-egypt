# ğŸ“¸ Ù†Ø¸Ø§Ù… Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø°ÙƒÙŠ Ø¨Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ - AI Visual Inspection
## Xchange AI-Powered Product Grading & Condition Assessment

**Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ©:** ğŸ”¥ Ø¹Ø§Ù„ÙŠØ© Ø¬Ø¯Ø§Ù‹
**Ø§Ù„ØªØ£Ø«ÙŠØ±:** +40% trust, -60% disputes
**ØµØ¹ÙˆØ¨Ø© Ø§Ù„ØªØ·ÙˆÙŠØ±:** Ø¹Ø§Ù„ÙŠØ©
**Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ù…Ù‚Ø¯Ø±:** 10-12 Ø£Ø³Ø¨ÙˆØ¹

---

## ğŸ“‹ Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ù…Ø­ØªÙˆÙŠØ§Øª

1. [Ù†Ø¸Ø±Ø© Ø¹Ø§Ù…Ø©](#overview)
2. [Ø§Ù„Ù…ÙˆØ§ØµÙØ§Øª Ø§Ù„ØªÙ‚Ù†ÙŠØ©](#technical-specs)
3. [Database Schema](#database-schema)
4. [API Endpoints](#api-endpoints)
5. [AI Models & Algorithms](#ai-models)
6. [Mobile SDK Integration](#mobile-sdk)
7. [User Stories](#user-stories)
8. [Implementation Guide](#implementation)
9. [Model Training](#training)

---

## 1. Ù†Ø¸Ø±Ø© Ø¹Ø§Ù…Ø© {#overview}

### 1.1 Ø§Ù„Ù…Ø´ÙƒÙ„Ø©

**Ø§Ù„ØªØ­Ø¯ÙŠØ§Øª Ø§Ù„Ø­Ø§Ù„ÙŠØ©:**
- **70%** Ù…Ù† Ø§Ù„Ù†Ø²Ø§Ø¹Ø§Øª Ø¨Ø³Ø¨Ø¨ "Ø§Ù„Ù…Ù†ØªØ¬ Ù„Ø§ ÙŠØ·Ø§Ø¨Ù‚ Ø§Ù„ÙˆØµÙ"
- Ø§Ù„Ø¨Ø§Ø¦Ø¹ÙˆÙ† ÙŠØ¨Ø§Ù„ØºÙˆÙ† ÙÙŠ ÙˆØµÙ Ø§Ù„Ø­Ø§Ù„Ø©
- Ø§Ù„Ù…Ø´ØªØ±ÙˆÙ† Ù„Ø§ ÙŠØ«Ù‚ÙˆÙ† ÙÙŠ Ø§Ù„ØµÙˆØ±
- Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù…Ø¹ÙŠØ§Ø± Ù…ÙˆØ­Ø¯ Ù„Ù„ØªÙ‚ÙŠÙŠÙ…
- ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø­Ø§Ù„Ø© Ø´Ø®ØµÙŠ ÙˆÙ…ØªØ­ÙŠØ²

### 1.2 Ø§Ù„Ø­Ù„

**Ù†Ø¸Ø§Ù… Ø°ÙƒÙŠ Ù…ØªÙƒØ§Ù…Ù„ ÙŠØ³ØªØ®Ø¯Ù… Computer Vision Ù„Ù€:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Ù‚Ø¯Ø±Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø°ÙƒÙŠ                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                       â”‚
â”‚  âœ… ØªÙ‚ÙŠÙŠÙ… ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù„Ù„Ø­Ø§Ù„Ø© (A/B/C/D)                   â”‚
â”‚  âœ… ÙƒØ´Ù Ø§Ù„Ø¹ÙŠÙˆØ¨ ÙˆØ§Ù„Ø®Ø¯ÙˆØ´                              â”‚
â”‚  âœ… Ù‚ÙŠØ§Ø³ Ù†Ø³Ø¨Ø© Ø§Ù„ØªÙ„Ù                                 â”‚
â”‚  âœ… Ù…Ù‚Ø§Ø±Ù†Ø© Ù…Ø¹ Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„ØµÙ†Ø§Ø¹Ø©                        â”‚
â”‚  âœ… Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Ù…ØµÙˆØ± ØªÙØµÙŠÙ„ÙŠ                         â”‚
â”‚  âœ… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø³Ø¹Ø± Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø­Ø§Ù„Ø©                   â”‚
â”‚  âœ… Ø¶Ù…Ø§Ù† Ø§Ù„Ø´ÙØ§ÙÙŠØ© ÙˆØ§Ù„Ø«Ù‚Ø©                            â”‚
â”‚                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.3 ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª Ø§Ù„Ù…Ø¯Ø¹ÙˆÙ…Ø©

| Ø§Ù„ÙØ¦Ø© | Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ | Ø§Ù„Ø¯Ù‚Ø© Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø© |
|------|---------|----------------|
| **ğŸ“± Mobiles** | Custom YOLOv8 | 92% |
| **ğŸš— Vehicles** | ResNet-50 | 88% |
| **ğŸ’ Luxury Items** | EfficientNet | 90% |
| **âš™ï¸ Electronics** | MobileNetV3 | 85% |
| **ğŸª™ Gold/Silver** | Custom CNN | 94% |

### 1.4 Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„ØªÙ‚ÙŠÙŠÙ…

```typescript
enum ConditionGrade {
  A = 'EXCELLENT',      // Ù…Ù…ØªØ§Ø²: 95-100% Ø¬Ø¯ÙŠØ¯
  B = 'VERY_GOOD',      // Ø¬ÙŠØ¯ Ø¬Ø¯Ø§Ù‹: 80-94%
  C = 'GOOD',           // Ø¬ÙŠØ¯: 60-79%
  D = 'FAIR',           // Ù…Ù‚Ø¨ÙˆÙ„: 40-59%
  E = 'POOR'            // Ø³ÙŠØ¡: < 40%
}

interface DefectType {
  SCRATCH: 'Ø®Ø¯Ø´';        // Severity: 1-10
  CRACK: 'Ø´Ø±Ø®/ÙƒØ³Ø±';
  DENT: 'Ø§Ù†Ø¨Ø¹Ø§Ø¬';
  DISCOLORATION: 'ØªØºÙŠØ± Ù„ÙˆÙ†';
  WEAR: 'ØªØ¢ÙƒÙ„';
  DAMAGE: 'ØªÙ„Ù';
}
```

---

## 2. Ø§Ù„Ù…ÙˆØ§ØµÙØ§Øª Ø§Ù„ØªÙ‚Ù†ÙŠØ© {#technical-specs}

### 2.1 System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   ARCHITECTURE                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚  ğŸ“± Mobile App                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                       â”‚
â”‚  â”‚  Camera UI   â”‚                                       â”‚
â”‚  â”‚  + Guides    â”‚â”€â”€â”€â”€â”€â”€â”                                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚                                â”‚
â”‚                        â–¼                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚  â”‚   On-Device Processing       â”‚                       â”‚
â”‚  â”‚   â€¢ Image Quality Check      â”‚                       â”‚
â”‚  â”‚   â€¢ Auto-crop & Enhance      â”‚                       â”‚
â”‚  â”‚   â€¢ Compression               â”‚                       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚                â”‚                                         â”‚
â”‚                â–¼                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚  â”‚      Upload to Cloud         â”‚                       â”‚
â”‚  â”‚   (AWS S3 + CloudFront)      â”‚                       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚                â”‚                                         â”‚
â”‚                â–¼                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚         AI Processing Pipeline       â”‚               â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤               â”‚
â”‚  â”‚  1. Object Detection (YOLOv8)        â”‚               â”‚
â”‚  â”‚  2. Defect Detection                 â”‚               â”‚
â”‚  â”‚  3. Condition Classification         â”‚               â”‚
â”‚  â”‚  4. Quality Scoring                  â”‚               â”‚
â”‚  â”‚  5. Report Generation                â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                â”‚                                         â”‚
â”‚                â–¼                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚  â”‚      PostgreSQL DB           â”‚                       â”‚
â”‚  â”‚  + Inspection Results        â”‚                       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 Technology Stack

| Layer | Technology | Purpose |
|-------|------------|---------|
| **Mobile** | React Native + Vision Camera | ØµÙˆØ± Ø¹Ø§Ù„ÙŠØ© Ø§Ù„Ø¬ÙˆØ¯Ø© |
| **Image Processing** | Sharp.js, ImageMagick | Ù…Ø¹Ø§Ù„Ø¬Ø© ÙˆØªØ­Ø³ÙŠÙ† |
| **AI/ML Framework** | PyTorch, ONNX Runtime | ØªØ´ØºÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ |
| **Object Detection** | YOLOv8, Faster R-CNN | ÙƒØ´Ù Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª |
| **Defect Detection** | Custom CNN, Detectron2 | ÙƒØ´Ù Ø§Ù„Ø¹ÙŠÙˆØ¨ |
| **Cloud ML** | AWS SageMaker, Lambda | Inference |
| **Storage** | AWS S3, CloudFront CDN | ØªØ®Ø²ÙŠÙ† Ø§Ù„ØµÙˆØ± |
| **Database** | PostgreSQL + JSONB | Ø§Ù„Ù†ØªØ§Ø¦Ø¬ |

### 2.3 Hardware Requirements

**Server:**
- GPU: NVIDIA T4 Ø£Ùˆ Ø£ÙØ¶Ù„ (Ù„Ù„Ù€ Inference)
- RAM: 16GB+
- Storage: 500GB SSD

**Mobile:**
- Camera: 12MP+
- RAM: 4GB+
- OS: iOS 13+ / Android 10+

---

## 3. Database Schema {#database-schema}

```prisma
// ============================================
// AI VISUAL INSPECTION SCHEMA
// ============================================

enum InspectionStatus {
  PENDING          // ÙÙŠ Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø±
  PROCESSING       // Ø¬Ø§Ø±ÙŠ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
  COMPLETED        // Ù…ÙƒØªÙ…Ù„
  FAILED           // ÙØ´Ù„
  REVIEW_NEEDED    // ÙŠØ­ØªØ§Ø¬ Ù…Ø±Ø§Ø¬Ø¹Ø© Ø¨Ø´Ø±ÙŠØ©
}

enum ConditionGrade {
  A  // Excellent: 95-100%
  B  // Very Good: 80-94%
  C  // Good: 60-79%
  D  // Fair: 40-59%
  E  // Poor: < 40%
}

enum DefectSeverity {
  MINOR      // Ø¨Ø³ÙŠØ·: Ù„Ø§ ÙŠØ¤Ø«Ø± Ø¹Ù„Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙØ©
  MODERATE   // Ù…ØªÙˆØ³Ø·: ÙŠØ¤Ø«Ø± Ù‚Ù„ÙŠÙ„Ø§Ù‹
  MAJOR      // ÙƒØ¨ÙŠØ±: ÙŠØ¤Ø«Ø± Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
  CRITICAL   // Ø­Ø±Ø¬: ØºÙŠØ± Ù‚Ø§Ø¨Ù„ Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
}

model VisualInspection {
  id                  String              @id @default(uuid())

  // Reference
  listingId           String              @unique
  listing             Listing             @relation(fields: [listingId], references: [id])
  userId              String
  user                User                @relation(fields: [userId], references: [id])

  // Status
  status              InspectionStatus    @default(PENDING)

  // Images Uploaded
  images              InspectionImage[]
  totalImages         Int                 @default(0)
  requiredImages      Int                 // Ø¹Ø¯Ø¯ Ø§Ù„ØµÙˆØ± Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ø­Ø³Ø¨ Ø§Ù„ÙØ¦Ø©

  // AI Results
  overallGrade        ConditionGrade?
  overallScore        Float?              // 0-100
  confidence          Float?              // 0-100

  // Defects Summary
  totalDefects        Int                 @default(0)
  minorDefects        Int                 @default(0)
  moderateDefects     Int                 @default(0)
  majorDefects        Int                 @default(0)
  criticalDefects     Int                 @default(0)

  // Detailed Analysis
  defects             Defect[]
  qualityMetrics      Json?               // {clarity, lighting, completeness}

  // Price Impact
  originalPrice       Float?              // Ø§Ù„Ø³Ø¹Ø± Ù‚Ø¨Ù„ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…
  suggestedPrice      Float?              // Ø§Ù„Ø³Ø¹Ø± Ø§Ù„Ù…Ù‚ØªØ±Ø­ Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…
  priceAdjustment     Float?              // Ø§Ù„ØªØ¹Ø¯ÙŠÙ„ (%)

  // Report
  reportUrl           String?             // PDF report
  reportGenerated     Boolean             @default(false)

  // Human Review
  needsReview         Boolean             @default(false)
  reviewReason        String?
  reviewedBy          String?
  reviewedAt          DateTime?
  humanGrade          ConditionGrade?

  // Processing Info
  processingStarted   DateTime?
  processingCompleted DateTime?
  processingDuration  Int?                // milliseconds
  aiModel             String?             // yolov8-mobile-v1
  aiVersion           String?

  // Metadata
  metadata            Json?

  createdAt           DateTime            @default(now())
  updatedAt           DateTime            @updatedAt

  @@index([listingId])
  @@index([userId])
  @@index([status])
  @@index([overallGrade])
}

model InspectionImage {
  id                  String              @id @default(uuid())

  inspectionId        String
  inspection          VisualInspection    @relation(fields: [inspectionId], references: [id], onDelete: Cascade)

  // Image Details
  imageUrl            String              // S3 URL
  thumbnailUrl        String?
  imageType           String              // front, back, top, bottom, screen, etc.
  sequenceNumber      Int                 // ØªØ±ØªÙŠØ¨ Ø§Ù„ØµÙˆØ±Ø©
  isRequired          Boolean             @default(true)

  // Quality Metrics
  resolution          String?             // 1920x1080
  fileSize            Int?                // bytes
  format              String?             // jpg, png
  qualityScore        Float?              // 0-100

  // Quality Issues
  isBlurry            Boolean             @default(false)
  isDark              Boolean             @default(false)
  hasGlare            Boolean             @default(false)
  isCropped           Boolean             @default(false)

  // AI Processing
  processed           Boolean             @default(false)
  processingError     String?
  aiAnnotations       Json?               // Bounding boxes, labels

  // Defects found in this image
  defectsFound        Defect[]

  uploadedAt          DateTime            @default(now())

  @@index([inspectionId])
  @@index([imageType])
}

model Defect {
  id                  String              @id @default(uuid())

  inspectionId        String
  inspection          VisualInspection    @relation(fields: [inspectionId], references: [id], onDelete: Cascade)

  imageId             String
  image               InspectionImage     @relation(fields: [imageId], references: [id])

  // Defect Details
  type                String              // scratch, crack, dent, etc.
  typeAr              String              // Ø®Ø¯Ø´ØŒ Ø´Ø±Ø®ØŒ Ø§Ù†Ø¨Ø¹Ø§Ø¬
  severity            DefectSeverity
  location            String              // front_top_left, screen, etc.

  // Bounding Box (normalized 0-1)
  boundingBox         Json                // {x, y, width, height}

  // Measurements
  area                Float?              // mmÂ² or pixelsÂ²
  length              Float?              // mm
  depth               Float?              // mm

  // Impact
  affectsFunctionality Boolean           @default(false)
  affectsAesthetics    Boolean           @default(true)
  priceImpact         Float?              // % reduction

  // AI Confidence
  confidence          Float               // 0-100
  aiModel             String

  // Description
  description         String?
  descriptionAr       String?

  detectedAt          DateTime            @default(now())

  @@index([inspectionId])
  @@index([severity])
  @@index([type])
}

// Pre-defined inspection templates
model InspectionTemplate {
  id                  String              @id @default(uuid())

  category            String              // mobiles, vehicles, etc.
  name                String
  nameAr              String

  // Required Images
  requiredImages      Json                // [{type, angle, description}]
  minImages           Int
  maxImages           Int

  // Inspection Points
  checkpoints         Json                // [{area, whatToCheck, weight}]

  // AI Model to use
  modelName           String
  modelVersion        String

  // Grading Criteria
  gradingCriteria     Json                // {A: {min, max}, B: {...}}

  isActive            Boolean             @default(true)

  createdAt           DateTime            @default(now())
  updatedAt           DateTime            @updatedAt

  @@unique([category, name])
}

// Analytics
model InspectionAnalytics {
  id                  String              @id @default(uuid())

  inspectionId        String              @unique

  // Performance Metrics
  totalProcessingTime Int                 // ms
  imageProcessingTime Int                 // ms
  modelInferenceTime  Int                 // ms

  // Accuracy (if human review available)
  humanGrade          ConditionGrade?
  aiGrade             ConditionGrade
  wasAccurate         Boolean?

  // User Feedback
  userAccepted        Boolean?
  userDisputed        Boolean?
  disputeReason       String?

  createdAt           DateTime            @default(now())

  @@index([wasAccurate])
  @@index([userAccepted])
}
```

---

## 4. API Endpoints {#api-endpoints}

### 4.1 Start Inspection

```typescript
POST /api/inspections/start

Request:
{
  "listingId": "uuid",
  "category": "mobiles",
  "brand": "iPhone",
  "model": "14 Pro"
}

Response:
{
  "success": true,
  "data": {
    "inspectionId": "uuid",
    "requiredImages": [
      {
        "type": "front",
        "description": "ØµÙˆØ±Ø© Ø§Ù„Ø¬Ù‡Ø§Ø² Ù…Ù† Ø§Ù„Ø£Ù…Ø§Ù…",
        "example": "https://cdn.../example-front.jpg",
        "tips": ["ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¶ÙˆØ­ Ø§Ù„Ø´Ø§Ø´Ø©", "Ø¥Ø¶Ø§Ø¡Ø© Ø¬ÙŠØ¯Ø©"]
      },
      {
        "type": "back",
        "description": "ØµÙˆØ±Ø© Ù…Ù† Ø§Ù„Ø®Ù„Ù",
        "tips": ["Ø£Ø¸Ù‡Ø± Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ Ø¨ÙˆØ¶ÙˆØ­", "Ø£ÙŠ Ø®Ø¯ÙˆØ´ Ù…Ø±Ø¦ÙŠØ©"]
      },
      {
        "type": "screen_on",
        "description": "Ø§Ù„Ø´Ø§Ø´Ø© Ù…ÙØªÙˆØ­Ø© (Ø´Ø§Ø´Ø© Ø¨ÙŠØ¶Ø§Ø¡)",
        "tips": ["Ø§ÙØªØ­ ØµÙˆØ±Ø© Ø¨ÙŠØ¶Ø§Ø¡ ÙƒØ§Ù…Ù„Ø©", "Ù„Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„Ø¨Ù‚Ø¹"]
      },
      {
        "type": "edges",
        "description": "Ø§Ù„Ø­ÙˆØ§Ù ÙˆØ§Ù„Ø¬ÙˆØ§Ù†Ø¨",
        "tips": ["ØµÙˆØ± Ù‚Ø±ÙŠØ¨Ø© Ù„Ù„Ø­ÙˆØ§Ù"]
      },
      {
        "type": "ports",
        "description": "Ø§Ù„Ù…Ù†Ø§ÙØ° (Ø´Ø­Ù†ØŒ Ø³Ù…Ø§Ø¹Ø§Øª)",
        "tips": ["Ù‚Ø±Ø¨ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§", "ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ù„ÙˆØ¶ÙˆØ­"]
      }
    ],
    "minImages": 5,
    "maxImages": 10,
    "uploadUrl": "https://s3.../upload",
    "expiresAt": "2024-12-17T12:00:00Z"
  }
}
```

### 4.2 Upload Image

```typescript
POST /api/inspections/:id/images

Headers:
Content-Type: multipart/form-data

Body:
{
  "image": <File>,
  "imageType": "front",
  "sequenceNumber": 1
}

Response:
{
  "success": true,
  "data": {
    "imageId": "uuid",
    "imageUrl": "https://cdn.../image.jpg",
    "qualityCheck": {
      "passed": true,
      "score": 92,
      "issues": [],
      "warnings": ["Ø§Ù„Ø¥Ø¶Ø§Ø¡Ø© ÙŠÙ…ÙƒÙ† Ø£Ù† ØªÙƒÙˆÙ† Ø£ÙØ¶Ù„"]
    }
  }
}

// Ø¥Ø°Ø§ Ø§Ù„ØµÙˆØ±Ø© Ø±Ø¯ÙŠØ¦Ø©:
{
  "success": false,
  "error": {
    "code": "POOR_QUALITY",
    "message": "Ø¬ÙˆØ¯Ø© Ø§Ù„ØµÙˆØ±Ø© ØºÙŠØ± ÙƒØ§ÙÙŠØ©",
    "issues": [
      {
        "type": "BLURRY",
        "severity": "high",
        "message": "Ø§Ù„ØµÙˆØ±Ø© ØºÙŠØ± ÙˆØ§Ø¶Ø­Ø©. Ø­Ø§ÙˆÙ„ Ø§Ù„ØªØ«Ø¨ÙŠØª Ø£ÙƒØ«Ø±"
      },
      {
        "type": "DARK",
        "severity": "medium",
        "message": "Ø§Ù„Ø¥Ø¶Ø§Ø¡Ø© Ø¶Ø¹ÙŠÙØ©. Ø§Ø³ØªØ®Ø¯Ù… Ø¶ÙˆØ¡ Ø£ÙØ¶Ù„"
      }
    ],
    "suggestions": [
      "Ø§Ù…Ø³Ø­ Ø¹Ø¯Ø³Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§",
      "Ø§Ø³ØªØ®Ø¯Ù… ÙƒÙ„Ø§ Ø§Ù„ÙŠØ¯ÙŠÙ† Ù„Ù„ØªØ«Ø¨ÙŠØª",
      "Ø§Ù‚ØªØ±Ø¨ Ù…Ù† Ù…ØµØ¯Ø± Ø¶ÙˆØ¡"
    ]
  }
}
```

### 4.3 Process Inspection

```typescript
POST /api/inspections/:id/process

Response:
{
  "success": true,
  "data": {
    "status": "PROCESSING",
    "estimatedTime": 45,  // seconds
    "message": "Ø¬Ø§Ø±ÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ± Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ..."
  }
}
```

### 4.4 Get Results

```typescript
GET /api/inspections/:id/results

Response:
{
  "success": true,
  "data": {
    "inspectionId": "uuid",
    "status": "COMPLETED",

    // Overall Assessment
    "assessment": {
      "grade": "B",
      "gradeAr": "Ø¬ÙŠØ¯ Ø¬Ø¯Ø§Ù‹",
      "score": 87.5,
      "confidence": 94.2,
      "summary": "Ø§Ù„Ø¬Ù‡Ø§Ø² ÙÙŠ Ø­Ø§Ù„Ø© Ø¬ÙŠØ¯Ø© Ø¬Ø¯Ø§Ù‹ Ù…Ø¹ Ø¨Ø¹Ø¶ Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø®ÙÙŠÙ"
    },

    // Defects Found
    "defects": [
      {
        "id": "def-1",
        "type": "SCRATCH",
        "typeAr": "Ø®Ø¯Ø´",
        "severity": "MINOR",
        "location": "back_bottom_right",
        "locationAr": "Ø§Ù„Ø®Ù„Ù - Ø£Ø³ÙÙ„ Ø§Ù„ÙŠÙ…ÙŠÙ†",
        "description": "Ø®Ø¯Ø´ Ø³Ø·Ø­ÙŠ Ø·ÙÙŠÙ",
        "imageUrl": "https://cdn.../annotated-1.jpg",
        "boundingBox": { /* ... */ },
        "measurements": {
          "length": "5mm",
          "depth": "surface"
        },
        "impact": {
          "functionality": false,
          "aesthetics": true,
          "priceReduction": 1.5  // %
        }
      },
      {
        "id": "def-2",
        "type": "WEAR",
        "typeAr": "ØªØ¢ÙƒÙ„",
        "severity": "MINOR",
        "location": "edges",
        "description": "ØªØ¢ÙƒÙ„ Ø·ÙÙŠÙ ÙÙŠ Ø§Ù„Ø­ÙˆØ§Ù (Ø·Ø¨ÙŠØ¹ÙŠ Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…)",
        "impact": {
          "priceReduction": 0.5
        }
      }
    ],

    // Quality Breakdown
    "qualityMetrics": {
      "screen": {
        "score": 95,
        "status": "Ù…Ù…ØªØ§Ø²",
        "issues": []
      },
      "body": {
        "score": 85,
        "status": "Ø¬ÙŠØ¯ Ø¬Ø¯Ø§Ù‹",
        "issues": ["Ø®Ø¯ÙˆØ´ Ø³Ø·Ø­ÙŠØ© Ø·ÙÙŠÙØ©"]
      },
      "functionality": {
        "score": 100,
        "status": "ÙƒÙ„ Ø§Ù„ÙˆØ¸Ø§Ø¦Ù ØªØ¹Ù…Ù„"
      },
      "cleanliness": {
        "score": 90,
        "status": "Ù†Ø¸ÙŠÙ"
      }
    },

    // Price Impact
    "pricing": {
      "originalEstimate": 9000,
      "revisedEstimate": 8730,
      "adjustment": -3.0,  // %
      "reason": "Ø®ØµÙ… Ø¨Ø³ÙŠØ· Ø¨Ø³Ø¨Ø¨ Ø§Ù„Ø®Ø¯ÙˆØ´ Ø§Ù„Ø³Ø·Ø­ÙŠØ©",
      "marketComparison": {
        "avgPriceForGradeB": 8650,
        "yourPriceVsMarket": "+0.9%"  // Ø£Ø¹Ù„Ù‰ Ù‚Ù„ÙŠÙ„Ø§Ù‹
      }
    },

    // Recommendations
    "recommendations": {
      "forSeller": [
        "Ø§Ù„Ø³Ø¹Ø± Ø§Ù„Ù…Ù‚ØªØ±Ø­ Ø¹Ø§Ø¯Ù„ ÙˆØªÙ†Ø§ÙØ³ÙŠ",
        "Ø§Ø°ÙƒØ± Ø£Ù† Ø§Ù„Ø¬Ù‡Ø§Ø² ÙÙŠ Ø­Ø§Ù„Ø© Ù…Ù…ØªØ§Ø²Ø©",
        "Ø§Ù„ØªÙ‚Ø±ÙŠØ± ÙŠØ²ÙŠØ¯ Ø§Ù„Ø«Ù‚Ø© ÙˆÙŠØ³Ø±Ø¹ Ø§Ù„Ø¨ÙŠØ¹"
      ],
      "forBuyer": [
        "Ø§Ù„Ø­Ø§Ù„Ø© ÙƒÙ…Ø§ Ù…ÙˆØ¶Ø­Ø© ÙÙŠ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…",
        "Ø®Ø¯ÙˆØ´ Ø·ÙÙŠÙØ© ÙÙ‚Ø· - Ù„Ø§ ØªØ¤Ø«Ø± Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…",
        "Ø³Ø¹Ø± Ø¹Ø§Ø¯Ù„ Ù„Ù„Ø­Ø§Ù„Ø©"
      ]
    },

    // Report
    "report": {
      "pdfUrl": "https://cdn.../report.pdf",
      "shareUrl": "https://xchange.com/inspection/share/xxx",
      "validUntil": "2025-01-17T00:00:00Z"
    },

    "processedAt": "2024-12-17T10:30:00Z",
    "processingTime": 42  // seconds
  }
}
```

### 4.5 Request Human Review

```typescript
POST /api/inspections/:id/request-review

Body:
{
  "reason": "Ø£Ø¹ØªÙ‚Ø¯ Ø£Ù† Ø§Ù„ØªÙ‚ÙŠÙŠÙ… ØºÙŠØ± Ø¯Ù‚ÙŠÙ‚",
  "details": "Ø§Ù„Ø®Ø¯Ø´ Ù„ÙŠØ³ Ø¨Ù‡Ø°Ø§ Ø§Ù„ÙˆØ¶ÙˆØ­"
}

Response:
{
  "success": true,
  "data": {
    "reviewId": "uuid",
    "status": "REVIEW_PENDING",
    "estimatedTime": "24 hours",
    "message": "ØªÙ… Ø¥Ø±Ø³Ø§Ù„ Ø·Ù„Ø¨ Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹Ø©. Ø³ÙŠØªÙ… Ø§Ù„Ø±Ø¯ Ø®Ù„Ø§Ù„ 24 Ø³Ø§Ø¹Ø©"
  }
}
```

---

## 5. AI Models & Algorithms {#ai-models}

### 5.1 Object Detection (YOLOv8)

```python
# models/object_detector.py

import torch
from ultralytics import YOLO
import cv2
import numpy as np

class ProductDetector:
    def __init__(self, model_path: str):
        self.model = YOLO(model_path)
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self.model.to(self.device)

    def detect(self, image_path: str) -> dict:
        """
        ÙƒØ´Ù Ø§Ù„Ù…Ù†ØªØ¬ ÙÙŠ Ø§Ù„ØµÙˆØ±Ø©
        Returns: {bbox, confidence, class}
        """

        # Read image
        img = cv2.imread(image_path)

        # Run inference
        results = self.model(img)

        # Parse results
        detections = []
        for result in results:
            boxes = result.boxes
            for box in boxes:
                detection = {
                    'bbox': box.xyxy[0].tolist(),  # [x1, y1, x2, y2]
                    'confidence': float(box.conf[0]),
                    'class': int(box.cls[0]),
                    'class_name': self.model.names[int(box.cls[0])]
                }
                detections.append(detection)

        return {
            'detections': detections,
            'image_size': img.shape[:2]
        }

    def crop_product(self, image_path: str, output_path: str) -> str:
        """
        Ù‚Øµ Ø§Ù„ØµÙˆØ±Ø© Ù„Ù„Ù…Ù†ØªØ¬ ÙÙ‚Ø· (Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø®Ù„ÙÙŠØ©)
        """

        result = self.detect(image_path)
        if not result['detections']:
            return image_path  # Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ù†ØªØ¬

        # Ø£Ø®Ø° Ø£ÙƒØ¨Ø± detection
        main_detection = max(
            result['detections'],
            key=lambda x: x['confidence']
        )

        # Ù‚Øµ Ø§Ù„ØµÙˆØ±Ø©
        img = cv2.imread(image_path)
        x1, y1, x2, y2 = map(int, main_detection['bbox'])

        # Ø¥Ø¶Ø§ÙØ© padding
        padding = 20
        x1 = max(0, x1 - padding)
        y1 = max(0, y1 - padding)
        x2 = min(img.shape[1], x2 + padding)
        y2 = min(img.shape[0], y2 + padding)

        cropped = img[y1:y2, x1:x2]
        cv2.imwrite(output_path, cropped)

        return output_path
```

### 5.2 Defect Detection

```python
# models/defect_detector.py

import torch
import torch.nn as nn
from torchvision import models, transforms
from PIL import Image

class DefectDetector:
    """
    ÙƒØ´Ù Ø§Ù„Ø¹ÙŠÙˆØ¨ (Ø®Ø¯ÙˆØ´ØŒ Ø´Ø±ÙˆØ®ØŒ ØªØ¢ÙƒÙ„ØŒ Ø¥Ù„Ø®)
    Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Faster R-CNN
    """

    def __init__(self, model_path: str, num_classes: int = 10):
        # Load pre-trained Faster R-CNN
        self.model = models.detection.fasterrcnn_resnet50_fpn(
            pretrained=False,
            num_classes=num_classes
        )

        # Load fine-tuned weights
        checkpoint = torch.load(model_path)
        self.model.load_state_dict(checkpoint['model_state_dict'])

        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model.to(self.device)
        self.model.eval()

        # Class names
        self.classes = [
            'background',
            'scratch',
            'crack',
            'dent',
            'discoloration',
            'wear',
            'chip',
            'stain',
            'corrosion',
            'deformation'
        ]

        self.classes_ar = {
            'scratch': 'Ø®Ø¯Ø´',
            'crack': 'Ø´Ø±Ø®',
            'dent': 'Ø§Ù†Ø¨Ø¹Ø§Ø¬',
            'discoloration': 'ØªØºÙŠØ± Ù„ÙˆÙ†',
            'wear': 'ØªØ¢ÙƒÙ„',
            'chip': 'Ù‚Ø´Ø±Ø©',
            'stain': 'Ø¨Ù‚Ø¹Ø©',
            'corrosion': 'ØµØ¯Ø£',
            'deformation': 'ØªØ´ÙˆÙ‡'
        }

        # Transform
        self.transform = transforms.Compose([
            transforms.ToTensor(),
        ])

    def detect_defects(
        self,
        image_path: str,
        confidence_threshold: float = 0.6
    ) -> list:
        """
        ÙƒØ´Ù Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¹ÙŠÙˆØ¨ ÙÙŠ Ø§Ù„ØµÙˆØ±Ø©
        """

        # Load image
        img = Image.open(image_path).convert('RGB')
        img_tensor = self.transform(img).unsqueeze(0).to(self.device)

        # Inference
        with torch.no_grad():
            predictions = self.model(img_tensor)

        # Parse predictions
        pred = predictions[0]
        boxes = pred['boxes'].cpu().numpy()
        labels = pred['labels'].cpu().numpy()
        scores = pred['scores'].cpu().numpy()

        defects = []

        for box, label, score in zip(boxes, labels, scores):
            if score < confidence_threshold:
                continue

            class_name = self.classes[label]
            if class_name == 'background':
                continue

            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ø³Ø§Ø­Ø©
            area = (box[2] - box[0]) * (box[3] - box[1])

            # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø´Ø¯Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø­Ø¬Ù… ÙˆØ§Ù„Ù†ÙˆØ¹
            severity = self._calculate_severity(class_name, area, score)

            defect = {
                'type': class_name,
                'typeAr': self.classes_ar.get(class_name, class_name),
                'bbox': box.tolist(),
                'confidence': float(score),
                'area': float(area),
                'severity': severity,
                'location': self._determine_location(box, img.size)
            }

            defects.append(defect)

        return defects

    def _calculate_severity(
        self,
        defect_type: str,
        area: float,
        confidence: float
    ) -> str:
        """
        Ø­Ø³Ø§Ø¨ Ø´Ø¯Ø© Ø§Ù„Ø¹ÙŠØ¨
        """

        # Ø¹ÙŠÙˆØ¨ Ø­Ø±Ø¬Ø© Ø¯Ø§Ø¦Ù…Ø§Ù‹
        critical_defects = ['crack', 'deformation', 'corrosion']
        if defect_type in critical_defects:
            return 'CRITICAL' if area > 1000 else 'MAJOR'

        # Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø³Ø§Ø­Ø©
        if area > 2000:
            return 'MAJOR'
        elif area > 500:
            return 'MODERATE'
        else:
            return 'MINOR'

    def _determine_location(self, bbox, image_size) -> str:
        """
        ØªØ­Ø¯ÙŠØ¯ Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø¹ÙŠØ¨ ÙÙŠ Ø§Ù„ØµÙˆØ±Ø©
        """

        width, height = image_size
        x_center = (bbox[0] + bbox[2]) / 2
        y_center = (bbox[1] + bbox[3]) / 2

        # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„ØµÙˆØ±Ø© Ù„Ù€ 9 Ø£Ù‚Ø³Ø§Ù…
        x_third = width / 3
        y_third = height / 3

        x_pos = 'left' if x_center < x_third else 'center' if x_center < 2 * x_third else 'right'
        y_pos = 'top' if y_center < y_third else 'middle' if y_center < 2 * y_third else 'bottom'

        return f"{y_pos}_{x_pos}"
```

### 5.3 Condition Grading

```python
# models/condition_grader.py

import numpy as np
from typing import Dict, List

class ConditionGrader:
    """
    ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ø¹Ø§Ù…Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø¹ÙŠÙˆØ¨ Ø§Ù„Ù…ÙƒØªØ´ÙØ©
    """

    def __init__(self):
        # Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ø´Ø¯Ø©
        self.severity_weights = {
            'MINOR': 1,
            'MODERATE': 3,
            'MAJOR': 8,
            'CRITICAL': 20
        }

        # Ø£ÙˆØ²Ø§Ù† Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¹ÙŠÙˆØ¨
        self.defect_weights = {
            'scratch': 1.0,
            'wear': 0.8,
            'stain': 0.7,
            'discoloration': 1.2,
            'dent': 1.5,
            'chip': 1.8,
            'crack': 3.0,
            'corrosion': 2.5,
            'deformation': 3.0
        }

    def calculate_grade(
        self,
        defects: List[Dict],
        category: str = 'general'
    ) -> Dict:
        """
        Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¯Ø±Ø¬Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
        """

        if not defects:
            return {
                'grade': 'A',
                'score': 100,
                'confidence': 100,
                'summary': 'Ù…Ù…ØªØ§Ø² - Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¹ÙŠÙˆØ¨ Ù…Ù„Ø­ÙˆØ¸Ø©'
            }

        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ù…Ø®ØµÙˆÙ…Ø©
        total_deduction = 0

        for defect in defects:
            severity_weight = self.severity_weights.get(
                defect['severity'],
                1
            )
            defect_weight = self.defect_weights.get(
                defect['type'],
                1.0
            )

            # Ø§Ù„Ù…Ø³Ø§Ø­Ø© ÙƒØ¹Ø§Ù…Ù„
            area_factor = min(defect.get('area', 100) / 1000, 2.0)

            # Ø§Ù„Ø®ØµÙ…
            deduction = severity_weight * defect_weight * area_factor
            total_deduction += deduction

        # Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
        final_score = max(0, 100 - total_deduction)

        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¯Ø±Ø¬Ø©
        if final_score >= 95:
            grade = 'A'
            grade_text = 'Ù…Ù…ØªØ§Ø²'
        elif final_score >= 80:
            grade = 'B'
            grade_text = 'Ø¬ÙŠØ¯ Ø¬Ø¯Ø§Ù‹'
        elif final_score >= 60:
            grade = 'C'
            grade_text = 'Ø¬ÙŠØ¯'
        elif final_score >= 40:
            grade = 'D'
            grade_text = 'Ù…Ù‚Ø¨ÙˆÙ„'
        else:
            grade = 'E'
            grade_text = 'Ø³ÙŠØ¡'

        # Ù…Ù„Ø®Øµ
        defect_counts = self._count_by_severity(defects)
        summary = self._generate_summary(grade, defect_counts)

        # Ø§Ù„Ø«Ù‚Ø©
        confidence = self._calculate_confidence(defects)

        return {
            'grade': grade,
            'gradeText': grade_text,
            'score': round(final_score, 1),
            'confidence': round(confidence, 1),
            'summary': summary,
            'defectCounts': defect_counts
        }

    def _count_by_severity(self, defects: List[Dict]) -> Dict:
        counts = {
            'MINOR': 0,
            'MODERATE': 0,
            'MAJOR': 0,
            'CRITICAL': 0
        }

        for defect in defects:
            severity = defect.get('severity', 'MINOR')
            counts[severity] += 1

        return counts

    def _generate_summary(self, grade: str, counts: Dict) -> str:
        if grade == 'A':
            return "Ù…Ù…ØªØ§Ø² - Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¹ÙŠÙˆØ¨ Ù…Ù„Ø­ÙˆØ¸Ø©"
        elif grade == 'B':
            if counts['MINOR'] > 0:
                return f"Ø¬ÙŠØ¯ Ø¬Ø¯Ø§Ù‹ - Ø¨Ø¹Ø¶ Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø·ÙÙŠÙØ© ({counts['MINOR']} Ø¹ÙŠÙˆØ¨ Ø¨Ø³ÙŠØ·Ø©)"
            else:
                return "Ø¬ÙŠØ¯ Ø¬Ø¯Ø§Ù‹ - Ø­Ø§Ù„Ø© Ù…Ù…ØªØ§Ø²Ø© Ù…Ø¹ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø®ÙÙŠÙ"
        elif grade == 'C':
            return f"Ø¬ÙŠØ¯ - Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ§Ø¶Ø­Ø© ({counts['MODERATE']} Ø¹ÙŠÙˆØ¨ Ù…ØªÙˆØ³Ø·Ø©)"
        elif grade == 'D':
            return f"Ù…Ù‚Ø¨ÙˆÙ„ - ÙŠØ­ØªØ§Ø¬ Ø¹Ù†Ø§ÙŠØ© ({counts['MAJOR']} Ø¹ÙŠÙˆØ¨ ÙƒØ¨ÙŠØ±Ø©)"
        else:
            return "Ø³ÙŠØ¡ - Ø¹ÙŠÙˆØ¨ ÙƒØ«ÙŠØ±Ø© ØªØ¤Ø«Ø± Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…"

    def _calculate_confidence(self, defects: List[Dict]) -> float:
        """
        Ø­Ø³Ø§Ø¨ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø«Ù‚Ø© ÙÙŠ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…
        """

        if not defects:
            return 100.0

        # Ù…ØªÙˆØ³Ø· Ø«Ù‚Ø© ÙƒØ´Ù Ø§Ù„Ø¹ÙŠÙˆØ¨
        avg_confidence = np.mean([d['confidence'] for d in defects]) * 100

        # Ø¹Ø¯Ø¯ Ø§Ù„ØµÙˆØ± Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø© (Ù…Ù† metadata)
        # ÙƒÙ„Ù…Ø§ Ø²Ø§Ø¯ Ø¹Ø¯Ø¯ Ø§Ù„ØµÙˆØ±ØŒ Ø²Ø§Ø¯Øª Ø§Ù„Ø«Ù‚Ø©

        return min(avg_confidence, 100.0)
```

### 5.4 Image Quality Check

```python
# utils/image_quality.py

import cv2
import numpy as np
from PIL import Image

class ImageQualityChecker:
    """
    ÙØ­Øµ Ø¬ÙˆØ¯Ø© Ø§Ù„ØµÙˆØ±Ø© Ù‚Ø¨Ù„ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
    """

    def __init__(self):
        self.min_resolution = (800, 600)
        self.max_file_size = 10 * 1024 * 1024  # 10MB
        self.min_brightness = 50
        self.max_brightness = 230

    def check_quality(self, image_path: str) -> Dict:
        """
        ÙØ­Øµ Ø´Ø§Ù…Ù„ Ù„Ù„ØµÙˆØ±Ø©
        """

        issues = []
        warnings = []
        score = 100

        # 1. ÙØ­Øµ Ø§Ù„Ø­Ø¬Ù…
        img = Image.open(image_path)
        width, height = img.size

        if width < self.min_resolution[0] or height < self.min_resolution[1]:
            issues.append({
                'type': 'LOW_RESOLUTION',
                'severity': 'high',
                'message': f'Ø§Ù„ØµÙˆØ±Ø© ØµØºÙŠØ±Ø© Ø¬Ø¯Ø§Ù‹ ({width}x{height}). ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„ {self.min_resolution[0]}x{self.min_resolution[1]}'
            })
            score -= 30

        # 2. ÙØ­Øµ Ø§Ù„ÙˆØ¶ÙˆØ­ (Blurriness)
        img_cv = cv2.imread(image_path)
        gray = cv2.cvtColor(img_cv, cv2.COLOR_BGR2GRAY)
        laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()

        if laplacian_var < 100:
            issues.append({
                'type': 'BLURRY',
                'severity': 'high',
                'message': 'Ø§Ù„ØµÙˆØ±Ø© ØºÙŠØ± ÙˆØ§Ø¶Ø­Ø©. Ø«Ø¨Øª Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ Ø¬ÙŠØ¯Ø§Ù‹'
            })
            score -= 25
        elif laplacian_var < 200:
            warnings.append('Ø§Ù„ØµÙˆØ±Ø© ÙŠÙ…ÙƒÙ† Ø£Ù† ØªÙƒÙˆÙ† Ø£ÙˆØ¶Ø­ Ù‚Ù„ÙŠÙ„Ø§Ù‹')
            score -= 5

        # 3. ÙØ­Øµ Ø§Ù„Ø¥Ø¶Ø§Ø¡Ø©
        brightness = np.mean(gray)

        if brightness < self.min_brightness:
            issues.append({
                'type': 'DARK',
                'severity': 'medium',
                'message': 'Ø§Ù„Ø¥Ø¶Ø§Ø¡Ø© Ø¶Ø¹ÙŠÙØ© Ø¬Ø¯Ø§Ù‹. Ø§Ø³ØªØ®Ø¯Ù… Ø¥Ø¶Ø§Ø¡Ø© Ø£ÙØ¶Ù„'
            })
            score -= 20
        elif brightness > self.max_brightness:
            issues.append({
                'type': 'OVEREXPOSED',
                'severity': 'medium',
                'message': 'Ø¥Ø¶Ø§Ø¡Ø© Ø²Ø§Ø¦Ø¯Ø©. Ù‚Ù„Ù„ Ø§Ù„Ø¥Ø¶Ø§Ø¡Ø© Ù‚Ù„ÙŠÙ„Ø§Ù‹'
            })
            score -= 15

        # 4. ÙØ­Øµ Ø§Ù„ÙˆÙ‡Ø¬ (Glare)
        hsv = cv2.cvtColor(img_cv, cv2.COLOR_BGR2HSV)
        v_channel = hsv[:, :, 2]
        bright_pixels = np.sum(v_channel > 250)
        bright_ratio = bright_pixels / (width * height)

        if bright_ratio > 0.1:
            warnings.append('ÙŠÙˆØ¬Ø¯ ÙˆÙ‡Ø¬ ÙÙŠ Ø§Ù„ØµÙˆØ±Ø©. ØªØ¬Ù†Ø¨ Ø§Ù„Ø§Ù†Ø¹ÙƒØ§Ø³Ø§Øª')
            score -= 10

        # 5. ÙØ­Øµ Ø§Ù„Ø£Ù„ÙˆØ§Ù†
        if img.mode != 'RGB':
            issues.append({
                'type': 'COLOR_MODE',
                'severity': 'low',
                'message': 'ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ø£Ù„ÙˆØ§Ù† ØºÙŠØ± Ù…Ø«Ø§Ù„ÙŠ'
            })
            score -= 5

        # Ø§Ù„Ù†ØªÙŠØ¬Ø©
        passed = len([i for i in issues if i['severity'] == 'high']) == 0

        return {
            'passed': passed,
            'score': max(0, score),
            'issues': issues,
            'warnings': warnings,
            'metrics': {
                'resolution': f'{width}x{height}',
                'brightness': round(brightness, 1),
                'sharpness': round(laplacian_var, 1)
            }
        }
```

---

## 6. Mobile SDK Integration {#mobile-sdk}

### 6.1 React Native Camera Component

```typescript
// components/InspectionCamera.tsx

import React, { useState, useRef } from 'react';
import { Camera, useCameraDevice } from 'react-native-vision-camera';
import { StyleSheet, View, Text, TouchableOpacity } from 'react-native';

interface InspectionCameraProps {
  imageType: string;
  description: string;
  tips: string[];
  onCapture: (uri: string) => void;
}

export function InspectionCamera({
  imageType,
  description,
  tips,
  onCapture
}: InspectionCameraProps) {

  const device = useCameraDevice('back');
  const camera = useRef<Camera>(null);
  const [isCapturing, setIsCapturing] = useState(false);

  const capturePhoto = async () => {
    if (!camera.current) return;

    setIsCapturing(true);

    try {
      const photo = await camera.current.takePhoto({
        flash: 'off',
        qualityPrioritization: 'quality',
        enableShutterSound: true
      });

      // Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø­Ù„ÙŠØ© Ø³Ø±ÙŠØ¹Ø©
      const processedUri = await processImageLocally(photo.path);

      onCapture(processedUri);

    } catch (error) {
      console.error('Capture failed:', error);
    } finally {
      setIsCapturing(false);
    }
  };

  if (!device) {
    return <Text>Loading camera...</Text>;
  }

  return (
    <View style={styles.container}>

      {/* Camera View */}
      <Camera
        ref={camera}
        style={StyleSheet.absoluteFill}
        device={device}
        isActive={true}
        photo={true}
      />

      {/* Overlay Guide */}
      <View style={styles.overlay}>

        {/* Top Instructions */}
        <View style={styles.topBar}>
          <Text style={styles.description}>{description}</Text>
        </View>

        {/* Framing Guide */}
        <View style={styles.frameGuide}>
          <View style={styles.corner} />
          <View style={[styles.corner, styles.topRight]} />
          <View style={[styles.corner, styles.bottomLeft]} />
          <View style={[styles.corner, styles.bottomRight]} />
        </View>

        {/* Bottom Tips & Capture Button */}
        <View style={styles.bottomBar}>

          {/* Tips */}
          <View style={styles.tips}>
            {tips.map((tip, index) => (
              <Text key={index} style={styles.tipText}>
                ğŸ’¡ {tip}
              </Text>
            ))}
          </View>

          {/* Capture Button */}
          <TouchableOpacity
            style={styles.captureButton}
            onPress={capturePhoto}
            disabled={isCapturing}
          >
            <View style={styles.captureButtonInner} />
          </TouchableOpacity>

        </View>

      </View>

    </View>
  );
}

async function processImageLocally(uri: string): Promise<string> {
  // Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨Ø³ÙŠØ·Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø¬Ù‡Ø§Ø²:
  // - ØªØµØ­ÙŠØ­ Ø§Ù„ØªÙˆØ¬ÙŠÙ‡
  // - Ø¶ØºØ· Ø®ÙÙŠÙ
  // - ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ù„ÙˆØ§Ù†

  // TODO: Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ÙƒØªØ¨Ø© Ù…Ø«Ù„ react-native-image-manipulator

  return uri;
}

const styles = StyleSheet.create({
  container: {
    flex: 1,
  },
  overlay: {
    flex: 1,
    backgroundColor: 'transparent',
  },
  topBar: {
    padding: 20,
    backgroundColor: 'rgba(0,0,0,0.6)',
  },
  description: {
    color: 'white',
    fontSize: 18,
    fontWeight: 'bold',
    textAlign: 'center',
  },
  frameGuide: {
    flex: 1,
    margin: 40,
    borderWidth: 2,
    borderColor: 'rgba(255,255,255,0.5)',
    borderStyle: 'dashed',
  },
  corner: {
    position: 'absolute',
    width: 30,
    height: 30,
    borderColor: '#00ff00',
    borderWidth: 3,
    borderTopWidth: 3,
    borderLeftWidth: 3,
    borderRightWidth: 0,
    borderBottomWidth: 0,
    top: -2,
    left: -2,
  },
  topRight: {
    left: undefined,
    right: -2,
    borderLeftWidth: 0,
    borderRightWidth: 3,
  },
  bottomLeft: {
    top: undefined,
    bottom: -2,
    borderTopWidth: 0,
    borderBottomWidth: 3,
  },
  bottomRight: {
    top: undefined,
    bottom: -2,
    left: undefined,
    right: -2,
    borderTopWidth: 0,
    borderBottomWidth: 3,
    borderLeftWidth: 0,
    borderRightWidth: 3,
  },
  bottomBar: {
    padding: 20,
    backgroundColor: 'rgba(0,0,0,0.6)',
    alignItems: 'center',
  },
  tips: {
    marginBottom: 20,
  },
  tipText: {
    color: '#ffeb3b',
    fontSize: 14,
    marginBottom: 5,
  },
  captureButton: {
    width: 70,
    height: 70,
    borderRadius: 35,
    backgroundColor: 'white',
    justifyContent: 'center',
    alignItems: 'center',
  },
  captureButtonInner: {
    width: 60,
    height: 60,
    borderRadius: 30,
    backgroundColor: '#2196f3',
  },
});
```

---

## 7. User Stories {#user-stories}

### Story 1: Ø§Ù„Ø¨Ø§Ø¦Ø¹ ÙŠÙ„ØªÙ‚Ø· ØµÙˆØ± Ø§Ù„Ù…Ù†ØªØ¬

```
ÙƒÙ€ Ø¨Ø§Ø¦Ø¹
Ø£Ø±ÙŠØ¯ ØªØµÙˆÙŠØ± Ù…Ù†ØªØ¬ÙŠ Ø¨Ø³Ù‡ÙˆÙ„Ø©
Ø­ØªÙ‰ Ø£Ø­ØµÙ„ Ø¹Ù„Ù‰ ØªÙ‚ÙŠÙŠÙ… Ø¯Ù‚ÙŠÙ‚ ÙˆÙ…ÙˆØ«ÙˆÙ‚

Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„Ù‚Ø¨ÙˆÙ„:
âœ… ØªØ·Ø¨ÙŠÙ‚ ÙŠØ±Ø´Ø¯Ù†ÙŠ Ù„Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„ØµÙˆØ± Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
âœ… Ø¯Ù„Ø§Ø¦Ù„ Ù…Ø±Ø¦ÙŠØ© Ù„ÙˆØ¶Ø¹ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§
âœ… ÙØ­Øµ Ø¬ÙˆØ¯Ø© ÙÙˆØ±ÙŠ Ù„Ù„ØµÙˆØ±Ø©
âœ… Ù†ØµØ§Ø¦Ø­ Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„ØµÙˆØ±

Ø§Ù„Ù…Ø«Ø§Ù„:
1. Ø§Ù„Ø¨Ø§Ø¦Ø¹ ÙŠØ¶ØºØ· "Ø§Ø¨Ø¯Ø£ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…"
2. Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ ÙŠØ·Ù„Ø¨ 5 ØµÙˆØ± (Ø£Ù…Ø§Ù…ØŒ Ø®Ù„ÙØŒ Ø´Ø§Ø´Ø©ØŒ Ø­ÙˆØ§ÙØŒ Ù…Ù†Ø§ÙØ°)
3. Ø¹Ù†Ø¯ ÙƒÙ„ ØµÙˆØ±Ø©ØŒ Ø¥Ø·Ø§Ø± ÙŠÙˆØ¶Ø­ ÙƒÙŠÙ ÙŠØ¶Ø¹ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§
4. Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ‚Ø§Ø·ØŒ ÙØ­Øµ ÙÙˆØ±ÙŠ: "Ù…Ù…ØªØ§Ø²!" Ø£Ùˆ "ØºÙŠØ± ÙˆØ§Ø¶Ø­Ø©ØŒ Ø£Ø¹Ø¯ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø©"
5. Ø¹Ù†Ø¯ Ø¥ÙƒÙ…Ø§Ù„ ÙƒÙ„ Ø§Ù„ØµÙˆØ±ØŒ "Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„..."
```

### Story 2: Ø§Ø³ØªÙ„Ø§Ù… Ø§Ù„ØªÙ‚Ø±ÙŠØ±

```
ÙƒÙ€ Ø¨Ø§Ø¦Ø¹
Ø£Ø±ÙŠØ¯ Ø±Ø¤ÙŠØ© ØªÙ‚Ø±ÙŠØ± ÙˆØ§Ø¶Ø­ Ù„Ø­Ø§Ù„Ø© Ù…Ù†ØªØ¬ÙŠ
Ø­ØªÙ‰ Ø£ÙÙ‡Ù… Ø§Ù„ØªÙ‚ÙŠÙŠÙ… ÙˆØ£Ø¶Ø¨Ø· Ø§Ù„Ø³Ø¹Ø±

Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„Ù‚Ø¨ÙˆÙ„:
âœ… Ø¯Ø±Ø¬Ø© ÙˆØ§Ø¶Ø­Ø© (A/B/C/D/E) Ù…Ø¹ ØªÙØ³ÙŠØ±
âœ… Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø§Ù„Ø¹ÙŠÙˆØ¨ Ù…Ø¹ ØµÙˆØ± Ù…ÙˆØ¶Ø­Ø©
âœ… Ø³Ø¹Ø± Ù…Ù‚ØªØ±Ø­ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø­Ø§Ù„Ø©
âœ… Ø¥Ù…ÙƒØ§Ù†ÙŠØ© Ø§Ù„Ø§Ø¹ØªØ±Ø§Ø¶ ÙˆØ§Ù„Ø·Ù„Ø¨ Ù…Ø±Ø§Ø¬Ø¹Ø©

Ø§Ù„Ù…Ø«Ø§Ù„:
- "Ø¯Ø±Ø¬Ø© B - Ø¬ÙŠØ¯ Ø¬Ø¯Ø§Ù‹ (87.5%)"
- "Ø¹ÙŠØ¨Ø§Ù† ØµØºÙŠØ±Ø§Ù†: Ø®Ø¯Ø´ Ø³Ø·Ø­ÙŠØŒ ØªØ¢ÙƒÙ„ ÙÙŠ Ø§Ù„Ø­ÙˆØ§Ù"
- ØµÙˆØ± Ù…Ø¹ Ø£Ø³Ù‡Ù… ØªÙˆØ¶Ø­ Ø§Ù„Ø¹ÙŠÙˆØ¨
- "Ø§Ù„Ø³Ø¹Ø± Ø§Ù„Ù…Ù‚ØªØ±Ø­: 8,730 Ø¬.Ù… (Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† 9,000)"
- Ø²Ø± "Ø§Ø¹ØªØ±Ø§Ø¶ Ø¹Ù„Ù‰ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…" Ø¥Ø°Ø§ ØºÙŠØ± Ù…Ù‚ØªÙ†Ø¹
```

### Story 3: Ø§Ù„Ù…Ø´ØªØ±ÙŠ ÙŠØ«Ù‚ ÙÙŠ Ø§Ù„ØªÙ‚Ø±ÙŠØ±

```
ÙƒÙ€ Ù…Ø´ØªØ±ÙŠ
Ø£Ø±ÙŠØ¯ Ø±Ø¤ÙŠØ© ØªÙ‚ÙŠÙŠÙ… Ù…ÙˆØ«ÙˆÙ‚ Ù‚Ø¨Ù„ Ø§Ù„Ø´Ø±Ø§Ø¡
Ø­ØªÙ‰ Ø£ØªØ£ÙƒØ¯ Ù…Ù† Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ù†ØªØ¬

Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„Ù‚Ø¨ÙˆÙ„:
âœ… Ø¹Ø±Ø¶ Ø§Ù„Ø¯Ø±Ø¬Ø© ÙˆØ§Ù„Ø¹ÙŠÙˆØ¨
âœ… Ù…Ù‚Ø§Ø±Ù†Ø© Ø¨Ø§Ù„Ø³ÙˆÙ‚
âœ… ØªÙˆØµÙŠØ© ÙˆØ§Ø¶Ø­Ø©
âœ… Ø¶Ù…Ø§Ù† Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©

Ø§Ù„Ù…Ø«Ø§Ù„:
- Ø¹Ù†Ø¯ ÙØªØ­ Ø§Ù„Ø¥Ø¹Ù„Ø§Ù†: Ø´Ø§Ø±Ø© "âœ“ ØªÙ… Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ"
- "Ø¯Ø±Ø¬Ø© B Ù…Ø¹ Ø®Ø¯ÙˆØ´ Ø·ÙÙŠÙØ©"
- "Ø§Ù„Ø³Ø¹Ø± Ø¹Ø§Ø¯Ù„ Ù…Ù‚Ø§Ø±Ù†Ø© Ø¨Ø§Ù„Ø³ÙˆÙ‚"
- "Ø¥Ø°Ø§ Ø§Ø³ØªÙ„Ù…Øª Ø§Ù„Ù…Ù†ØªØ¬ ÙˆÙˆØ¬Ø¯Øª Ø¹ÙŠÙˆØ¨ Ø¥Ø¶Ø§ÙÙŠØ©ØŒ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ø¥Ø±Ø¬Ø§Ø¹"
```

---

## 8. Implementation Guide {#implementation}

### Phase 1: Infrastructure (Week 1-2)
```bash
# Setup
- AWS S3 buckets Ù„Ù„ØµÙˆØ±
- CloudFront CDN
- Lambda Ù„Ù„Ù€ image processing
- SageMaker endpoint Ù„Ù„Ù€ ML models
- Database migrations
```

### Phase 2: Base Models (Week 3-5)
```bash
# Train models
- YOLOv8 Ù„Ù„Ù€ object detection
- Faster R-CNN Ù„Ù„Ù€ defect detection
- Custom classifier Ù„Ù„Ù€ condition grading
- Test Ø¹Ù„Ù‰ 10k+ ØµÙˆØ±Ø©
```

### Phase 3: Backend API (Week 6-8)
```bash
# Develop
- Inspection endpoints
- Image upload/processing pipeline
- AI inference service
- Report generation
```

### Phase 4: Mobile Integration (Week 9-11)
```bash
# Mobile
- Camera component
- Upload UI
- Results display
- Offline caching
```

### Phase 5: Testing & Launch (Week 12)
```bash
# QA
- Beta testing with 100 users
- Model accuracy validation
- Performance optimization
- Public launch
```

---

## 9. Model Training {#training}

### Dataset Requirements

```
Total Images Needed: 50,000+

Breakdown:
- Mobiles: 15,000 images
  - iPhone: 5,000
  - Samsung: 5,000
  - Other: 5,000

- Vehicles: 20,000 images
  - Exterior: 10,000
  - Interior: 10,000

- Electronics: 10,000

- Luxury: 5,000

Annotations:
- Bounding boxes Ù„ÙƒÙ„ Ø¹ÙŠØ¨
- Labels (scratch, crack, etc.)
- Severity ratings
- Human-verified grades
```

### Training Pipeline

```python
# training/train_defect_detector.py

import torch
from torch.utils.data import DataLoader
import wandb

# Initialize W&B
wandb.init(project='xchange-visual-inspection')

# Load dataset
train_dataset = DefectDataset('data/train')
val_dataset = DefectDataset('data/val')

train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=8)

# Model
model = FasterRCNN(num_classes=10)
model.to(device)

# Optimizer
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# Training loop
for epoch in range(50):
    train_loss = train_epoch(model, train_loader, optimizer)
    val_loss, val_map = validate(model, val_loader)

    wandb.log({
        'train_loss': train_loss,
        'val_loss': val_loss,
        'val_mAP': val_map
    })

    # Save best model
    if val_map > best_map:
        torch.save(model.state_dict(), 'models/best_model.pth')
```

---

**ØªØ§Ø±ÙŠØ® Ø§Ù„Ø¥Ù†Ø´Ø§Ø¡:** Ø¯ÙŠØ³Ù…Ø¨Ø± 2024
**Ø§Ù„Ø¥ØµØ¯Ø§Ø±:** 1.0
**Ø§Ù„Ù…Ø·ÙˆØ±:** Xchange Egypt Platform
